# Инструкция для работы Data Engineer

Ниже — практическое руководство шаг за шагом: роль, обязанности, типичные рабочие процессы, набор инструментов, шаблоны и чек-листы. Подойдут как для начинающего, так и для опытного инженера — бери то, что нужно и адаптируй под свой стек.

---

# 1. Кратко — кто такой Data Engineer

Data Engineer (DE) отвечает за надёжный сбор, хранение, трансформацию и доставку данных так, чтобы аналитики, ML-инженеры и приложения могли их использовать. Цель — качественные, доступные и отслеживаемые дата-пайплайны с понятным SLA и стоимостью.

---

# 2. Ежедневные/типичные обязанности

* Поддержка и мониторинг продакшен-пайплайнов.
* Проектирование/оптимизация схем данных и ETL/ELT-процессов.
* Интеграция новых источников данных (APIs, стримы, базы, файлы).
* Настройка оркестрации задач (DAGs), планировщиков.
* Тестирование и CI/CD для дата-конфигураций и кодов трансформаций.
* Обеспечение качества данных: валидация, профайлинг, алерты.
* Документация схем, контрактов и SLA.
* Управление стоимостью хранения и вычислений.

---

# 3. Ключевые навыки и знания

* Языки: SQL (обязательно), Python/Scala/Java для трансформаций.
* Принципы: ETL vs ELT, CDC, idempotency, partitioning, schema evolution.
* СУБД/хранилища: OLTP (Postgres/MySQL), OLAP/warehouse (Snowflake, BigQuery, Redshift), Data Lake (S3/GCS/ADLS).
* Оркестрация: Airflow, Dagster, Prefect, Kubernetes CronJobs.
* Стриминг (если нужно): Kafka, Pulsar, Kinesis, Spark Streaming/Flink.
* Инструменты трансформации: dbt, Spark, Beam.
* Инфраструктура: Docker, Terraform, Helm, Kubernetes.
* Мониторинг/логирование: Prometheus/Grafana, ELK/EFK, Sentry.
* Практики тестирования: unit/ integration для ETL, тесты качества данных.
* Безопасность и GDPR/CCPA-совместность, управление схемами (schema registry).

---

# 4. Типичный рабочий процесс (шаблон пайплайна)

1. **Интеграция источника**

   * Выяснить контракт данных (схема, частота, гарантии).
   * Решить способ: batch (SFTP/HTTP) или stream (Kafka/API).
2. **Ингарест (ingest)**

   * Поставить лёгкий адаптер/collector; сохранять сырые данные в staging (raw layer).
   * Логировать метаданные: timestamp, source, offset/идентификатор.
3. **Сохранение сырого слоя**

   * Хранить неизменяемо (immutable) с версией схемы и partitioning.
4. **Трансформации (ELT предпочтительнее)**

   * Использовать dbt/Spark для бизнес-логики: raw -> staging -> curated/mart.
   * Делать трансформации идемпотентными; использовать CDC для уменьшения объёма.
5. **Валидация и тесты качества**

   * Проверки наличия/дубликатов/диапазонов/соотношений/NULLs.
   * Профайлинг и сохранение метрик (row counts, cardinality).
6. **Экспозиция**

   * Таблицы для аналитики (star schema), API/feature store для ML.
7. **Мониторинг и алерты**

   * Счётчики (успех/ошибка), SLA-метрики, задержки, рост стоимости.
8. **Документация**

   * Data contracts, схемы, lineage, примеры данных.

---

# 5. Best practices (кратко и по делу)

* Сделай всё **идемпотентным** — повторный запуск не ломает состояние.
* Храни сырьё **immutable**: raw layer = единый источник истины.
* Разделяй окружения: dev / staging / prod.
* Версионируй схему и контракты (schema registry, semantic versioning).
* Пиши **автоматические тесты** для трансформаций и проверок качества.
* Настрой **observability**: метрики, логи, трассировки, алерты.
* Минимизируй задержки и издержки: ретеншн, partitioning, clustering.
* Используй feature flags / миграции при изменении upstream схем.
* Документируй lineage и owner для каждой таблицы/пайплайна.
* Планируй восстановление данных и runbooks (что делать при падении).

---

# 6. Чек-лист на день / при запуске пайплайна

Ежедневно:

* Проверить статус DAG’ов / job’ов (успех/фейл).
* Проверить ключевые метрики: row counts, lateness, error rate.
* Просмотреть алерты и triage инцидентов.
* Проверить стоимость (биллинг) при аномалии нагрузки.

При запуске нового источника/пайплайна:

* Есть контракт данных и owner.
* Есть тестовые данные и интеграционные тесты.
* Стадии: raw → staging → curated настроены.
* Настроены мониторинг и алерты.
* Документация и runbook обновлены.
* Ревью безопасности: PII, шифрование, доступы.

---

# 7. Пример структуры репозитория (recommended)

```
/repo
  /infrastructure     # Terraform / k8s / helm
  /deployments        # CI/CD manifests
  /etl                # код трансформаций (python/spark)
  /dbt                # dbt проект (models, tests)
  /tests              # integration / data tests
  /docs               # схема, lineage, runbooks
  /scripts            # утилиты
  README.md
```

---

# 8. Простой пример CI/CD для dbt (фрагмент задачи)

(идея; адаптируй под свой CI)

```yaml
name: dbt CI
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with: python-version: '3.10'
      - name: Install deps
        run: pip install dbt-core dbt-postgres
      - name: Run dbt compile & test
        run: |
          dbt deps
          dbt seed --profiles-dir .
          dbt run --profiles-dir .
          dbt test --profiles-dir .
```

(Добавь прометей-метрики и алерты при сбое.)

---

# 9. Советы по отладке инцидентов

* Повтори работу пайплайна локально на тех же данных (используй snapshot тестовых данных).
* Сравни row counts между шага A и B; ищи резкие отличия.
* Проверяй offsets/токи (streaming): где остановился consumer?
* Просмотри схемные изменения upstream — часто ломает ETL.
* В runbook должен быть список быстрых шагов (rollback, reprocess window, contact owners).

---

# 10. Onboarding checklist для нового Data Engineer

* Доступы: Git, CI, monitoring, cloud console, data warehouse.
* Локальная dev среда: инструкция по запуску (docker-compose).
* Список активных пайплайнов + owners.
* Краткий walkthrough критичных DAG’ов и runbook’ы.
* Первая задача: небольшой багфикс + PR review (чтобы изучить кодовую базу).

---

# 11. Что можно улучшать постоянно

* Автоматизировать тесты качества данных.
* Внедрять data catalog (например, Amundsen/Atlas) и lineage.
* Оптимизировать стоимость хранилища и вычислений.
* Внедрять feature store для ML.
* Улучшать SLA и SLO, сокращать время восстановления (MTTR).

---

Если хочешь, могу:

* адаптировать инструкцию под конкретный стек (например, **Airflow + Snowflake + S3 + dbt**),
* подготовить шаблон runbook’а для одного из пайплайнов,
* или сгенерировать пример CI/CD pipeline для Kubernetes/Argo.

Какой вариант тебе полезнее прямо сейчас?
